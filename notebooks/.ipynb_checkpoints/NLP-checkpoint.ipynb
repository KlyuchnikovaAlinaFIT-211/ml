{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85add33",
   "metadata": {},
   "source": [
    "# Задачи\n",
    "1. Самостоятельно реализовать BoW, TF-IDF\n",
    "2. Решить задачу классификации с понижением размерности. Использовать самостоятельно реализованные модели из предыдущих ЛР.\n",
    "3. Решить задачу мягкой кластеризации (ТМ) с помощью LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb015807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мешок слов:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2]\n",
      "\n",
      "Уникальные слова:\n",
      "['великолепный', 'сериал', 'который', 'поможет', 'успокоить', 'нервы', 'при', 'любых', 'стрессах', 'и', 'просто', 'скрасит', 'серые', 'будни', 'пожалуй', 'если', 'бы', 'я', 'посмотрел', 'только', 'первые', 'пару', 'сезонов', 'этого', 'сериала', 'с', 'легкой', 'руки', 'написал', 'ему', 'положительную', 'рецензию', 'в', 'общем', 'создатели', 'не', 'вернут', 'всё', 'на', 'круги', 'своя', 'то', 'рейтинги', 'следующих', 'будут', 'становится', 'все', 'ниже', 'а', 'зрительская', 'аудитория', 'будет', 'меньше']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def create_bag_of_words(corpus):\n",
    "    word_freq = {}\n",
    "    for doc in corpus:\n",
    "        words = preprocess_text(doc)\n",
    "        for word in words:\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 0\n",
    "            word_freq[word] += 1\n",
    "\n",
    "    unique_words = list(word_freq.keys())\n",
    "\n",
    "    bag_of_words = []\n",
    "    for doc in corpus:\n",
    "        words = preprocess_text(doc)\n",
    "        vector = [0] * len(unique_words)\n",
    "        for i, word in enumerate(unique_words):\n",
    "            if word in words:\n",
    "                vector[i] = words.count(word)\n",
    "        bag_of_words.append(vector)\n",
    "\n",
    "    return bag_of_words, unique_words\n",
    "\n",
    "documents = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
    "\n",
    "\n",
    "\n",
    "bag_of_words, unique_words = create_bag_of_words(documents)\n",
    "\n",
    "print(\"Мешок слов:\")\n",
    "for vector in bag_of_words:\n",
    "    print(vector)\n",
    "\n",
    "print(\"\\nУникальные слова:\")\n",
    "print(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e20067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for document 1:\n",
      "Великолепный: 0.12093908432571038\n",
      "сериал,: 0.12093908432571038\n",
      "который: 0.12093908432571038\n",
      "поможет: 0.12093908432571038\n",
      "успокоить: 0.12093908432571038\n",
      "нервы: 0.12093908432571038\n",
      "при: 0.12093908432571038\n",
      "любых: 0.12093908432571038\n",
      "стрессах: 0.12093908432571038\n",
      "и: 0.07142857142857142\n",
      "просто: 0.12093908432571038\n",
      "скрасит: 0.12093908432571038\n",
      "серые: 0.12093908432571038\n",
      "будни: 0.12093908432571038\n",
      "\n",
      "TF-IDF for document 2:\n",
      "Пожалуй,: 0.08465735902799727\n",
      "если: 0.06438410362258905\n",
      "бы: 0.1287682072451781\n",
      "я: 0.1287682072451781\n",
      "посмотрел: 0.08465735902799727\n",
      "только: 0.08465735902799727\n",
      "первые: 0.08465735902799727\n",
      "пару: 0.08465735902799727\n",
      "сезонов: 0.06438410362258905\n",
      "этого: 0.06438410362258905\n",
      "сериала,: 0.08465735902799727\n",
      "с: 0.05\n",
      "легкой: 0.08465735902799727\n",
      "руки: 0.08465735902799727\n",
      "написал: 0.08465735902799727\n",
      "ему: 0.08465735902799727\n",
      "положительную: 0.08465735902799727\n",
      "рецензию: 0.08465735902799727\n",
      "\n",
      "TF-IDF for document 3:\n",
      "В: 0.04292273574839269\n",
      "общем,: 0.05643823935199818\n",
      "если: 0.04292273574839269\n",
      "создатели: 0.05643823935199818\n",
      "этого: 0.04292273574839269\n",
      "сериала: 0.04292273574839269\n",
      "не: 0.04292273574839269\n",
      "вернут: 0.05643823935199818\n",
      "всё: 0.05643823935199818\n",
      "на: 0.04292273574839269\n",
      "круги: 0.05643823935199818\n",
      "своя,: 0.05643823935199818\n",
      "то: 0.03333333333333333\n",
      "рейтинги: 0.05643823935199818\n",
      "следующих: 0.05643823935199818\n",
      "сезонов: 0.04292273574839269\n",
      "будут: 0.05643823935199818\n",
      "становится: 0.05643823935199818\n",
      "все: 0.11287647870399636\n",
      "ниже: 0.05643823935199818\n",
      "и: 0.06666666666666667\n",
      "ниже,: 0.05643823935199818\n",
      "а: 0.03333333333333333\n",
      "зрительская: 0.05643823935199818\n",
      "аудитория: 0.05643823935199818\n",
      "будет: 0.05643823935199818\n",
      "меньше: 0.05643823935199818\n",
      "меньше.: 0.05643823935199818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_tf(term, document):\n",
    "    word_count = len(document.split())\n",
    "    term_count = document.split().count(term)\n",
    "    tf = term_count / word_count\n",
    "    return tf\n",
    "\n",
    "def calculate_idf(term, documents):\n",
    "    document_count = len(documents)\n",
    "    term_occurrences = sum(1 for document in documents if term in document)\n",
    "    idf = math.log((document_count + 1) / (1 + term_occurrences)) + 1\n",
    "    return idf\n",
    "\n",
    "def calculate_tfidf(term, document, documents):\n",
    "    tf = calculate_tf(term, document)\n",
    "    idf = calculate_idf(term, documents)\n",
    "    tfidf = tf * idf\n",
    "    return tfidf\n",
    "\n",
    "def calculate_tfidf_for_documents(documents):\n",
    "    tfidf_documents = []\n",
    "    for document in documents:\n",
    "        tfidf_document = {}\n",
    "        document_terms = document.split()\n",
    "        for term in document_terms:\n",
    "            tfidf_document[term] = calculate_tfidf(term, document, documents)\n",
    "        tfidf_documents.append(tfidf_document)\n",
    "    return tfidf_documents\n",
    "\n",
    "documents =[\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
    "\n",
    "tfidf_documents = calculate_tfidf_for_documents(documents)\n",
    "for i, document in enumerate(tfidf_documents):\n",
    "    print(f\"TF-IDF for document {i+1}:\")\n",
    "    for term, tfidf in document.items():\n",
    "        print(f\"{term}: {tfidf}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9250cec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.112876</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.056438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.120939  0.120939  0.120939  0.120939  0.120939  0.120939  0.120939   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9   ...        46        47        48        49  \\\n",
       "0  0.120939  0.120939  0.066667  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.066667  ...  0.056438  0.112876  0.056438  0.056438   \n",
       "\n",
       "         50        51        52        53        54        55  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.033333  0.056438  0.056438  0.056438  0.056438  0.056438  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_matrix(document):\n",
    "    matrix = []\n",
    "    result = {}\n",
    "    for d in document:\n",
    "        result.update(d)\n",
    "    unique_words = list(result.keys())\n",
    "    for words in document:\n",
    "        vector = [0] * len(unique_words)\n",
    "        for i, word in enumerate(unique_words):\n",
    "            if word in words:\n",
    "                vector[i] = result[word]\n",
    "        matrix.append(vector)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "f = get_matrix(tfidf_documents)\n",
    "data = pd.DataFrame(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83283899",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/ML/data/spam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/ML/data/spam.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\machine_learning\\venv\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/ML/data/spam.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('D:/ML/data/spam.csv', encoding='latin-1')\n",
    "y = df[\"v1\"]\n",
    "X = list(df[\"v2\"])\n",
    "\n",
    "tfidf_documents = calculate_tfidf_for_documents(X)\n",
    "for i, document in enumerate(tfidf_documents):\n",
    "    print(f\"TF-IDF for document {i+1}:\")\n",
    "    for term, tfidf in document.items():\n",
    "        print(f\"{term}: {tfidf}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = get_matrix(tfidf_documents)\n",
    "data = pd.DataFrame(matr)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c441c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "maping = {\n",
    "    \"ham\" : 1,\n",
    "    \"spam\": 0\n",
    "}\n",
    "\n",
    "y = y.replace(maping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c617df",
   "metadata": {},
   "source": [
    "# 2 часть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsClassification:\n",
    "    @staticmethod\n",
    "    def accuracy(y_test, y_pred):\n",
    "        y_true, predictions = np.array(y_test), np.array(y_pred)\n",
    "        return len([x for x, y  in zip(y_true, predictions) if x  == y])/len(y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_test, y_pred):\n",
    "        y_true, predictions = np.array(y_test), np.array(y_pred)\n",
    "        TP = FP =TN =FN = 0\n",
    "        for test, pred in zip(y_true, predictions):\n",
    "            if (test == 1 and pred == 1):\n",
    "                TP += 1 \n",
    "            elif (test == 0 and pred == 0):\n",
    "                TN += 1\n",
    "            elif (test == 1 and pred == 0):\n",
    "                FN += 1\n",
    "            elif (test == 0 and pred == 1):\n",
    "                FP += 1\n",
    "        return [[TP, FP],\n",
    "                         [FN, TN]]\n",
    "    @staticmethod\n",
    "    def precision( y_test, y_pred):\n",
    "        matrix = MetricsClassification.confusion_matrix(y_test, y_pred)\n",
    "        TP = matrix[0][0]\n",
    "        FP = matrix[0][1]\n",
    "        return TP/(TP + FP)\n",
    "    @staticmethod\n",
    "    def recall(y_test, y_pred):\n",
    "        matrix = MetricsClassification.confusion_matrix(y_test, y_pred)\n",
    "        TP = matrix[0][0]\n",
    "        FN = matrix[1][0]\n",
    "        return TP/(TP + FN)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_score(y_test, y_pred):\n",
    "        recall_score = MetricsClassification.recall(y_test, y_pred)\n",
    "        precision_score = MetricsClassification.precision(y_test, y_pred)\n",
    "        return 2*(recall_score * precision_score)/ (recall_score+precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, num_components):\n",
    "    # Центрирование данных\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "    print(\"1\")\n",
    "    # Вычисление ковариационной матрицы\n",
    "    cov_matrix = np.cov(X_meaned, rowvar=False)\n",
    "    print(\"2\")\n",
    "    # Вычисление собственных значений и собственных векторов\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)\n",
    "    print(\"3\")\n",
    "    # Сортировка собственных значений в убывающем порядке\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalues = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:, sorted_index]\n",
    "    print(\"4\")\n",
    "    # Выбор нужного числа главных компонент\n",
    "    eigenvector_subset = sorted_eigenvectors[:, 0:num_components]\n",
    "    print(\"5\")\n",
    "    # Проецирование данных на главные компоненты\n",
    "    X_reduced = np.dot(eigenvector_subset.transpose(), X_meaned.transpose()).transpose()\n",
    "    print(\"6\")\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77305cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca(data, 100)\n",
    "print(\"pca complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = np.array(self.y_train)[indices]\n",
    "            most_common_label = np.bincount(k_nearest_labels).argmax()\n",
    "            predictions.append(most_common_label)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=4)\n",
    "knn = KNN(10)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "print(MetricsClassification.accuracy(y_test, predictions))\n",
    "print(MetricsClassification.confusion_matrix(y_test, predictions))\n",
    "print(MetricsClassification.precision(y_test, predictions))\n",
    "print(MetricsClassification.recall(y_test, predictions))\n",
    "print(MetricsClassification.f_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4af8a9",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af225e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32352823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/ML/data/spam.csv', encoding='latin-1')\n",
    "y = df[\"v1\"]\n",
    "X = df[\"v2\"]\n",
    "data_samples = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76716b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56576cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=20, \n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1264d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(lda, tf_vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0985b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97b45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cd3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
